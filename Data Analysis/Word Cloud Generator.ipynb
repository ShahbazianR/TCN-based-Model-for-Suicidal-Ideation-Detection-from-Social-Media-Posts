{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fkOeWakNlf7G"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","from keras.preprocessing.text import Tokenizer\n","from nltk.tokenize import word_tokenize\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nj-kJCVhmRBj"},"outputs":[],"source":["#Preprocessing function\n","def preprocessing(data_frame):\n","    ## Preprocessing\n","    # Removing URLs whithin the tweets\n","    data_frame[\"Text\"] = data_frame[\"Text\"].str.replace(r'\\s*https?://\\S+(\\s+|$)', ' ').str.strip()\n","    # Removing emails, hashtags and punctuations\n","    data_frame['Text'] = data_frame[\"Text\"].str.replace(r'\\S*@\\S*\\s?', ' ').str.strip()\n","    data_frame['Text'] = data_frame['Text'].str.replace(r'#\\S*\\s?', ' ').str.strip()\n","    data_frame['Text'] = data_frame['Text'].str.replace(r'[^\\w\\s]+', ' ').str.strip()\n","\n","    # Lowercase Text\n","    data_frame['Text'] = data_frame['Text'].str.lower()\n","\n","    # # Removing stopwords\n","    stop = stopwords.words('english')\n","    data_frame['Text'].apply(lambda x: [item for item in str(x) if item not in stop])\n","\n","    # Removing newline characters\n","    data_frame['Text'] = data_frame['Text'].str.rstrip()\n","\n","    # # Tokenizing Posts and counting the length of each post\n","    data_frame['Tokens'] = data_frame.apply(lambda row: word_tokenize(str(row['Text'])), axis=1)\n","    # data_frame['Length'] = data_frame.apply(lambda row: len(row['Tokens']), axis=1)\n","\n","    return data_frame"]},{"cell_type":"markdown","metadata":{"id":"IAm3ALD-l8rD"},"source":["### Twitter 10 000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VNNc4IoKlt96"},"outputs":[],"source":["Path_Twitter_1 = \"/Datasets/Twitter/twitter-suicidal_data_10000.csv\"\n","df = pd.read_csv(Path_Twitter_1)\n","df = df.rename(columns={'tweet':'Text', 'intention':'Label'})"]},{"cell_type":"markdown","metadata":{"id":"po5W5puWqKDY"},"source":["### Twitter Tendency"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MXLKqLCRqMBU"},"outputs":[],"source":["Path_Twitter_2 = \"/Datasets/suicidal-tendency-tweets.csv\"\n","df = pd.read_csv(Path_Twitter_2, encoding='latin-1', usecols=['tweet', 'intention'], nrows = 17142)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"mDizEedHrHeA"},"source":["### Reddit SNS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJQvCGIVrJct"},"outputs":[],"source":["Reddit_path = \"/Datasets/Reddit_non suicide  suicide/Suicide_Detection.csv\"\n","\n","df = pd.read_csv(Reddit_path, encoding='latin-1', usecols=['text', 'class'], nrows=20000)\n","df = df.rename(columns={'text': 'Text', 'class': 'Label'})\n","\n","label_dict = {'suicide': 1, 'non-suicide': 0}\n","df['Label'] = df['Label'].apply(lambda row: label_dict[row])\n","df['Label']\n","\n","df"]},{"cell_type":"markdown","metadata":{"id":"7EVGfp8YnEFd"},"source":["# Data Prepration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sTOfTIv_qgJF"},"outputs":[],"source":["df = df.rename(columns={'tweet':'Text', 'intention':'Label'})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ofiEbpgbl_EH"},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pVWUU256nCiH"},"outputs":[],"source":["df.isna().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uxqzElyNFFbR"},"outputs":[],"source":["suicidal = []\n","for item in range(len(df)):\n","  if df['Label'][item] == 1:\n","    suicidal.append(df['Text'][item])\n","\n","suicidal_df = pd.DataFrame(suicidal, columns=['Text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eW9eEUyoIuD2"},"outputs":[],"source":["suicidal_df = preprocessing(suicidal_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oNuGeQh4IYVv"},"outputs":[],"source":["suicidal_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zp5HmNyNnL07"},"outputs":[],"source":["text = \" \"\n","tokens = []\n","for item in range(len(suicidal_df)):\n","  for token in suicidal_df['Tokens'][item]:\n","    if len(token)>1:\n","      tokens.append(token)\n","\n","# text = \" \".join(txt for txt in df.Text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"frLO_2zSJJt1"},"outputs":[],"source":["text = \" \".join(txt for txt in tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hP3wzWbBJekX"},"outputs":[],"source":["text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aDnwJbMbn2RJ"},"outputs":[],"source":["word_cloud = WordCloud(collocations = False, background_color = 'white').generate(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0odHbR9xpvpe"},"outputs":[],"source":["plt.imshow(word_cloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyODl9WRWaVH7b5lC9d1CERQ","collapsed_sections":["IAm3ALD-l8rD","po5W5puWqKDY","mDizEedHrHeA"],"mount_file_id":"1TejVKQrpxKZivA53alt9SA1eGHAVnZ5v","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
