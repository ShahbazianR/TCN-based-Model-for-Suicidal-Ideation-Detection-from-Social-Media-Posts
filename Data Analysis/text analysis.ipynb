{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"sJYd9Oq99Aef"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JYvmGjh9NZfY"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","from keras.preprocessing.text import Tokenizer\n","from nltk.tokenize import word_tokenize\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hUKMQwobMRWP"},"outputs":[],"source":["#Preprocessing function\n","def preprocessing(data_frame):\n","    ## Preprocessing\n","    # Removing URLs whithin the tweets\n","    data_frame[\"Text\"] = data_frame[\"Text\"].str.replace(r'\\s*https?://\\S+(\\s+|$)', ' ').str.strip()\n","    # Removing emails, hashtags and punctuations\n","    data_frame['Text'] = data_frame[\"Text\"].str.replace(r'\\S*@\\S*\\s?', ' ').str.strip()\n","    data_frame['Text'] = data_frame['Text'].str.replace(r'#\\S*\\s?', ' ').str.strip()\n","    data_frame['Text'] = data_frame['Text'].str.replace(r'[^\\w\\s]+', ' ').str.strip()\n","\n","    # Lowercase Text\n","    data_frame['Text'] = data_frame['Text'].str.lower()\n","\n","    # # Removing stopwords\n","    stop = stopwords.words('english')\n","    data_frame['Text'].apply(lambda x: [item for item in str(x) if item not in stop])\n","\n","    # Removing newline characters\n","    data_frame['Text'] = data_frame['Text'].str.rstrip()\n","\n","    return data_frame"]},{"cell_type":"markdown","metadata":{"id":"vW8dA-dxQh_8"},"source":["# Datasets"]},{"cell_type":"markdown","metadata":{"id":"fa23X8Nk4K-O"},"source":["### Sampling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"clQdShYAV1SX"},"outputs":[],"source":["def sampling(df, key1, key2, pos_label, neg_label):\n","  sample = []\n","  pos = 3\n","  neg = 2\n","  for item in range(len(df)):\n","    if pos == 0 and neg == 0:\n","      break\n","    elif pos >0:\n","      if df[key2][item] == pos_label:\n","        sample.append([df[key1][item], df[key2][item]])\n","        pos = pos-1\n","    elif neg >0:\n","      if df[key2][item] == neg_label:\n","        sample.append([df[key1][item], df[key2][item]])\n","        neg -= 1\n","  \n","  print(sample)\n","  df_sample = pd.DataFrame(sample, columns={key1, key2})\n","  return df_sample"]},{"cell_type":"markdown","metadata":{"id":"IAm3ALD-l8rD"},"source":["### Twitter 10 000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VNNc4IoKlt96"},"outputs":[],"source":["Path_Twitter_1 = \"/Datasets/Twitter/twitter-suicidal_data_10000.csv\"\n","df = pd.read_csv(Path_Twitter_1)\n","df = df.rename(columns={'tweet':'Text', 'intention':'Label'})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pNT6pfpabhvA"},"outputs":[],"source":["suicidal = []\n","for item in range(len(df)):\n","  if df['Label'][item] == 1:\n","    suicidal.append([df['Text'][item], df['Label'][item]])\n","\n","suicidal_df = pd.DataFrame(suicidal, columns=['Text', 'Label'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJRP4tW-bnHC"},"outputs":[],"source":["df = suicidal_df\n","df"]},{"cell_type":"markdown","metadata":{"id":"po5W5puWqKDY"},"source":["### Twitter Tendency"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MXLKqLCRqMBU"},"outputs":[],"source":["Path_Twitter_2 = \"/MH ML project/Datasets/suicidal-tendency-tweets.csv\"\n","df = pd.read_csv(Path_Twitter_2, encoding='latin-1', usecols=['tweet', 'intention'], nrows = 17142)\n","df = df.rename(columns={'tweet':'Text', 'intention':'Label'})\n","df = preprocessing(df)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x05PvDjBxFRC"},"outputs":[],"source":["suicidal = []\n","for item in range(len(df)):\n","  if df['Label'][item] == 1:\n","    suicidal.append([df['Text'][item], df['Label'][item]])\n","\n","suicidal_df = pd.DataFrame(suicidal, columns=['Text', 'Label'])\n","df = suicidal_df\n","df"]},{"cell_type":"markdown","metadata":{"id":"mDizEedHrHeA"},"source":["### Reddit SNS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJQvCGIVrJct"},"outputs":[],"source":["Reddit_path = \"/Datasets/Reddit_non suicide  suicide/Suicide_Detection.csv\" \n","df = pd.read_csv(Reddit_path, encoding='latin-1', usecols=['text', 'class'], nrows=20000)\n","df = df.rename(columns={'text': 'Text', 'class': 'Label'})\n","\n","label_dict = {'suicide': 1, 'non-suicide': 0}\n","df['Label'] = df['Label'].apply(lambda row: label_dict[row])\n","df['Label']\n","\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8J1TbFdL087m"},"outputs":[],"source":["suicidal = []\n","for item in range(len(df)):\n","  if df['Label'][item] == 1:\n","    suicidal.append([df['Text'][item], df['Label'][item]])\n","\n","suicidal_df = pd.DataFrame(suicidal, columns=['Text', 'Label'])\n","df = suicidal_df\n","df"]},{"cell_type":"markdown","metadata":{"id":"DbvQkDj6WKp_"},"source":["# DATA sampling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6k36SqOSWBsQ"},"outputs":[],"source":["df_sample = sampling(df, \"text\", \"label\", \"SuicideWatch\", \"depression\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ko44W8hV_HR"},"outputs":[],"source":["df_sample = df_sample.sample(frac=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E2FQZKijVXZw"},"outputs":[],"source":["df_sample"]},{"cell_type":"markdown","metadata":{"id":"HcWL-TXEQoQB"},"source":["# Analyses"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qUMR96F2wWkO"},"outputs":[],"source":["from textblob import TextBlob"]},{"cell_type":"markdown","metadata":{"id":"xfI_wWroP8my"},"source":["## Sentiment analysis1_polarity and subjective"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hXr9sf-mNk1d"},"outputs":[],"source":["# !pip install textblob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TqlB-Z11R37O"},"outputs":[],"source":["df['polarity'] = df['Text'].apply(lambda x: TextBlob(x).polarity)\n","df['subjective'] = df['Text'].apply(lambda x: TextBlob(x).subjectivity)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sUZDnwrFSYlk"},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8gkHjE7aHFa"},"outputs":[],"source":["save_path = \"/Datasets/analysis/\"\n","df.to_csv(save_path+'Reddit_SD_polarity_subjective.csv')"]},{"cell_type":"markdown","metadata":{"id":"bRgNbxEIU5TL"},"source":["## bigrams/trigrams"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ISo4X9wS_4w"},"outputs":[],"source":["from nltk.corpus import stopwords\n","stopwords = stopwords.words('english')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BflA22wwTA5u"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","# c_vec = CountVectorizer(stop_words=stopwords, ngram_range=(2,2)) ## bigrams\n","c_vec = CountVectorizer(stop_words=stopwords, ngram_range=(3,3)) ##trigrams\n","# matrix of ngrams\n","ngrams = c_vec.fit_transform(df['Text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gN-If1NGT6dV"},"outputs":[],"source":["# count frequency of ngrams\n","count_values = ngrams.toarray().sum(axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hOUgIvyGUL9H"},"outputs":[],"source":["# list of ngrams\n","vocab = c_vec.vocabulary_\n","df_ngram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)).rename(columns={0: 'frequency', 1:'bigram/trigram'})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m2SsmNJxUjBF"},"outputs":[],"source":["df_ngram['polarity'] = df_ngram['bigram/trigram'].apply(lambda x: TextBlob(x).polarity)\n","df_ngram['subjective'] = df_ngram['bigram/trigram'].apply(lambda x: TextBlob(x).subjectivity)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f_h7_1qcUYJ4"},"outputs":[],"source":["df_ngram"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Br6oXzSYT6Gq"},"outputs":[],"source":["save_path = \"/Datasets/analysis/\"\n","# df_ngram.to_csv(save_path+'Reddit_SD_bigrams.csv')\n","df_ngram.to_csv(save_path+'Reddit_SD_trigrams.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uIdizFwjUn0v"},"outputs":[],"source":["words = []\n","for item in range(len(df_ngram)):\n","  if df_ngram.polarity[item] > 0:\n","    words.append(df_ngram['bigram/trigram'][item])\n","\n","words"]},{"cell_type":"markdown","metadata":{"id":"yyKruHpFa6iT"},"source":["## Topic Detection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jTBfHWHncNA-"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import NMF\n","from sklearn.pipeline import make_pipeline\n","tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords, ngram_range=(2,3))\n","nmf = NMF(n_components=5)\n","pipe = make_pipeline(tfidf_vectorizer, nmf)\n","pipe.fit(df['Text'])\n","def print_top_words(model, feature_names, n_top_words):\n","    for topic_idx, topic in enumerate(model.components_):\n","        message = \"Topic #%d: \" % topic_idx\n","        message += \", \".join([feature_names[i]\n","                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n","        print(message)\n","    print()\n","print_top_words(nmf, tfidf_vectorizer.get_feature_names(), n_top_words=3)"]},{"cell_type":"markdown","metadata":{"id":"CCJs_XTed0KW"},"source":["# Sentiments analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6yZwRqZd63X"},"outputs":[],"source":["nltk.download('vader_lexicon')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Ycvwu3adzh3"},"outputs":[],"source":["from nltk.sentiment import SentimentIntensityAnalyzer\n","sia = SentimentIntensityAnalyzer()\n","sia.polarity_scores(df['Text'][1000])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YSqtN532ejSe"},"outputs":[],"source":["semantic_3 = []\n","sia = SentimentIntensityAnalyzer()\n","for item in range(len(df['Text'])):\n","  text = df['Text'][item]\n","  polarity_score = sia.polarity_scores(text)\n","  semantic_3.append([ text, polarity_score['neg'], polarity_score['neu'], polarity_score['pos'], polarity_score['compound']])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1dvLfLqLdrkI"},"outputs":[],"source":["semantic_3 = pd.DataFrame(semantic_3, columns=['Text', 'neg','neu','pos','compound'])\n","semantic_3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EGmtBMpYeEUB"},"outputs":[],"source":["save_path = \"/Datasets/analysis/\"\n","semantic_3.to_csv(save_path+'Reddit_SD_semantic_3.csv')"]},{"cell_type":"markdown","metadata":{"id":"ZuMFU8B0g-Js"},"source":["# Multimodal sentiment analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NQ7dzCUHeepz"},"outputs":[],"source":["!pip install -q transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Agq7ItAqfftq"},"outputs":[],"source":["from transformers import pipeline\n","data = df['Text'][3000]\n","\n","from transformers import pipeline\n","classifier = pipeline(\"text-classification\", model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)\n","prediction = classifier(data, )\n","print(prediction)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wXLC2UEyfaVz"},"outputs":[],"source":["prediction[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6z6LhrBce8Xg"},"outputs":[],"source":["semantic_6 = []\n","classifier = pipeline(\"text-classification\", model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)\n","for item in range(len(df['Text'])):\n","  print(item)\n","  text = df['Text'][item]\n","  if len(text) > 512:\n","    hm = len(text)/512\n","    split_text = []\n","    for n in range(0,int(hm)+1):\n","      split_text.append(text[n*512:(n+1)*512])\n","    print(len(text),len(split_text), hm, int(hm))\n","\n","    preds = []\n","    for txt in split_text:\n","      prediction = classifier(txt, )\n","      preds.append([prediction[0][0]['score'], prediction[0][1]['score'], prediction[0][2]['score'],\n","                      prediction[0][3]['score'], prediction[0][4]['score'], prediction[0][5]['score']])\n","    print(preds)\n","    scores = [0, 0, 0, 0, 0, 0]\n","    for pred_ind in preds:\n","      scores[0] += pred_ind[0]\n","      scores[1] += pred_ind[1]\n","      scores[2] += pred_ind[2]\n","      scores[3] += pred_ind[3]\n","      scores[4] += pred_ind[4]\n","      scores[5] += pred_ind[5]\n","    for index in range(len(scores)):\n","      scores[index] = scores[index]/len(preds)\n","    print(scores)\n","    semantic_6.append([text, scores[0], scores[1], scores[2],\n","                  scores[3], scores[4], scores[5]])\n","  else:\n","    prediction = classifier(text, )\n","    semantic_6.append([ text, prediction[0][0]['score'], prediction[0][1]['score'], prediction[0][2]['score'],\n","                      prediction[0][3]['score'], prediction[0][4]['score'], prediction[0][5]['score']])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jp6j2UNagY3S"},"outputs":[],"source":["semantic_6 = pd.DataFrame(semantic_6, columns=['Text', 'sadness', 'joy', 'love', 'anger', 'fear', 'surprise'])\n","semantic_6"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R0HbyA5eg81k"},"outputs":[],"source":["save_path = \"/Datasets/analysis/\"\n","semantic_6.to_csv(save_path+'Reddit_SD_semantic_6.csv')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOBw5Bh11HhYO0/hqWtkIKr","collapsed_sections":["fa23X8Nk4K-O","IAm3ALD-l8rD","mDizEedHrHeA","8oRDzdEzLG8z","DbvQkDj6WKp_"],"mount_file_id":"1wmIN6ubDA9Xsi1MFXUIXjfn8cboU6d3Z","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
